"""
LLM Provider Factory

í…ŒìŠ¤íŠ¸/ê°œë°œ í™˜ê²½ì—ì„œëŠ” Geminië¥¼ ì‚¬ìš©í•˜ê³ ,
í”„ë¡œë•ì…˜/ë°ëª¨ í™˜ê²½ì—ì„œëŠ” Claudeë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìë™ ì „í™˜
"""
import logging
from typing import Optional
from langchain_core.language_models import BaseChatModel
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

from src.config.settings import settings

logger = logging.getLogger(__name__)


def get_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> BaseChatModel:
    """
    LLM ëª¨ë“œì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    í™˜ê²½ ë³€ìˆ˜ LLM_MODEì— ë”°ë¼:
    - "production", "prod", "demo" â†’ Claude (Anthropic)
    - ê·¸ ì™¸ (test, development) â†’ Gemini (Google)

    Args:
        temperature: LLM temperature (ê¸°ë³¸ê°’: settings.LLM_TEMPERATURE)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: settings.MAX_TOKENS)
        model: ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: ìë™ ì„ íƒ)

    Returns:
        BaseChatModel: LLM ì¸ìŠ¤í„´ìŠ¤

    Examples:
        >>> # í…ŒìŠ¤íŠ¸ í™˜ê²½ (LLM_MODE=test)
        >>> llm = get_llm()  # Gemini ë°˜í™˜
        >>>
        >>> # í”„ë¡œë•ì…˜ í™˜ê²½ (LLM_MODE=production)
        >>> llm = get_llm()  # Claude ë°˜í™˜
    """
    provider = settings.llm_provider
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.llm_model_name

    logger.info(f"ğŸ¤– LLM ì´ˆê¸°í™”: provider={provider}, model={model_name}")

    if provider == "anthropic":
        # Claude (Anthropic)
        if not settings.ANTHROPIC_API_KEY:
            logger.warning("âš ï¸ ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Geminië¡œ í´ë°±í•©ë‹ˆë‹¤.")
            provider = "google"
        else:
            return ChatAnthropic(
                model=model_name,
                temperature=temp,
                max_tokens=tokens,
                api_key=settings.ANTHROPIC_API_KEY,
            )

    if provider == "google":
        # Gemini (Google)
        if not settings.GEMINI_API_KEY:
            raise ValueError(
                "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
            )

        return ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temp,
            max_output_tokens=tokens,
            google_api_key=settings.GEMINI_API_KEY,
        )

    # Fallback (should not reach here)
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider}")


def get_claude_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatAnthropic:
    """
    ê°•ì œë¡œ Claude ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)

    ì‹œì—°ì´ë‚˜ íŠ¹ì • ê¸°ëŠ¥ì—ì„œ Claudeë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•  ë•Œ

    Args:
        temperature: LLM temperature
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        model: ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: CLAUDE_MODEL)

    Returns:
        ChatAnthropic: Claude ì¸ìŠ¤í„´ìŠ¤
    """
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.CLAUDE_MODEL

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    logger.info(f"ğŸ¯ Claude ê°•ì œ ì‚¬ìš©: model={model_name}")

    return ChatAnthropic(
        model=model_name,
        temperature=temp,
        max_tokens=tokens,
        api_key=settings.ANTHROPIC_API_KEY,
    )


def get_gemini_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatGoogleGenerativeAI:
    """
    ê°•ì œë¡œ Gemini ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)

    í…ŒìŠ¤íŠ¸ë‚˜ ë¹„ìš© ì ˆê°ì´ í•„ìš”í•  ë•Œ

    Args:
        temperature: LLM temperature
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        model: ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: GEMINI_MODEL)

    Returns:
        ChatGoogleGenerativeAI: Gemini ì¸ìŠ¤í„´ìŠ¤
    """
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.GEMINI_MODEL

    if not settings.GEMINI_API_KEY:
        raise ValueError(
            "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    logger.info(f"âš¡ Gemini ê°•ì œ ì‚¬ìš©: model={model_name}")

    return ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temp,
        max_output_tokens=tokens,
        google_api_key=settings.GEMINI_API_KEY,
    )
