"""
LLM Provider Factory

í…ŒìŠ¤íŠ¸/ê°œë°œ í™˜ê²½ì—ì„œëŠ” Geminië¥¼ ì‚¬ìš©í•˜ê³ ,
í”„ë¡œë•ì…˜/ë°ëª¨ í™˜ê²½ì—ì„œëŠ” Claudeë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìë™ ì „í™˜
"""
import asyncio
import logging
import threading
from functools import lru_cache
from typing import Optional

try:
    from redis.exceptions import ResponseError as RedisResponseError
except Exception:  # redisê°€ ì„ íƒì  ì˜ì¡´ì„±ì¸ í™˜ê²½ ì§€ì›
    RedisResponseError = None

from langchain_anthropic import ChatAnthropic
from langchain_core.caches import InMemoryCache
from langchain_core.language_models import BaseChatModel
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

from src.config.settings import settings

logger = logging.getLogger(__name__)

# ìºì‹œ ì´ˆê¸°í™” ìƒíƒœ í”Œë˜ê·¸
_cache_initialized = False


def _loop_token() -> str:
    """
    í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„(ë˜ëŠ” ìŠ¤ë ˆë“œ)ë¥¼ ì‹ë³„í•˜ê¸° ìœ„í•œ í† í° ë°˜í™˜.

    gRPC ê¸°ë°˜ LLMë“¤ì€ ì´ë²¤íŠ¸ ë£¨í”„ì— ì¢…ì†ì ì¸ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ,
    ì„œë¡œ ë‹¤ë¥¸ ë£¨í”„ ê°„ì— ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê³µìœ í•˜ë©´ 'Event loop is closed' ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ìŠ¤ë ˆë“œ ID ê¸°ì¤€ìœ¼ë¡œ ê²©ë¦¬
        return f"sync-{threading.get_ident()}"
    return f"loop-{id(loop)}"


@lru_cache(maxsize=16)
def _build_llm(
    provider: str,
    model_name: str,
    temperature: float,
    max_tokens: int,
    loop_token: str,
) -> BaseChatModel:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ìºì‹±í•©ë‹ˆë‹¤.

    ë™ì¼í•œ ì„¤ì •(provider, model, temperature, max_tokens, loop_token)ì— ëŒ€í•´ì„œëŠ”
    ìºì‹œëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™” ë¹„ìš©ì„ ì¤„ì…ë‹ˆë‹¤.
    """
    logger.info(
        "ğŸ¤– LLM ì´ˆê¸°í™”: provider=%s, model=%s, temperature=%s, max_tokens=%s, loop=%s",
        provider,
        model_name,
        temperature,
        max_tokens,
        loop_token,
    )

    if provider == "anthropic":
        # Claude í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”
        return ChatAnthropic(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.ANTHROPIC_API_KEY,
            default_headers={"anthropic-beta": "prompt-caching-2024-07-31"},
        )

    if provider == "google":
        return ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temperature,
            max_output_tokens=max_tokens,
            google_api_key=settings.GEMINI_API_KEY,
        )

    if provider == "openai":
        return ChatOpenAI(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.OPENAI_API_KEY,
        )

    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider}")


def get_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> BaseChatModel:
    """
    LLM ëª¨ë“œì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    ê¸°ë³¸ ìš°ì„ ìˆœìœ„: OpenAI â†’ Claude (Anthropic) â†’ Gemini (Google)
    - í™˜ê²½ ë³€ìˆ˜ LLM_MODEë¡œ providerë¥¼ ëª…ì‹œí•  ìˆ˜ ìˆìŒ
    - API í‚¤ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ í´ë°±
    """
    requested_provider = settings.llm_provider
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.llm_model_name

    provider = requested_provider

    # í´ë°± ì²´ì¸: OpenAI â†’ Claude â†’ Gemini
    if provider == "openai" and not settings.OPENAI_API_KEY:
        logger.warning(
            "âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Claudeë¡œ í´ë°±í•©ë‹ˆë‹¤."
        )
        provider = "anthropic"

    if provider == "anthropic" and not settings.ANTHROPIC_API_KEY:
        logger.warning(
            "âš ï¸ ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Geminië¡œ í´ë°±í•©ë‹ˆë‹¤."
        )
        provider = "google"

    # ìµœì¢… ê²€ì¦
    if provider == "openai" and not settings.OPENAI_API_KEY:
        raise ValueError(
            "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    if provider == "anthropic" and not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    if provider == "google" and not settings.GEMINI_API_KEY:
        raise ValueError(
            "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm(provider, model_name, float(temp), int(tokens), loop_token)


def get_claude_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatAnthropic:
    """ê°•ì œë¡œ Claude ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.CLAUDE_MODEL

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_openai_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatOpenAI:
    """ê°•ì œë¡œ OpenAI ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else "gpt-4o-mini"

    if not settings.OPENAI_API_KEY:
        raise ValueError(
            "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("openai", model_name, float(temp), int(tokens), loop_token)


def get_gemini_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatGoogleGenerativeAI:
    """ê°•ì œë¡œ Gemini ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.GEMINI_MODEL

    if not settings.GEMINI_API_KEY:
        raise ValueError(
            "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("google", model_name, float(temp), int(tokens), loop_token)


def get_research_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Research Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_router_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Router Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_strategy_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Strategy Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_portfolio_risk_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Portfolio/Risk Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_default_agent_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatOpenAI:
    """ê¸°ë³¸ ì—ì´ì „íŠ¸ LLM (GPT-5-mini)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "gpt-5-mini"  # GPT-5-mini

    if not settings.OPENAI_API_KEY:
        raise ValueError(
            "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("openai", model_name, float(temp), int(tokens), loop_token)


def initialize_semantic_cache():
    """
    RedisSemanticCacheë¥¼ ì´ˆê¸°í™”í•˜ì—¬ LLM ì‘ë‹µ ìºì‹±ì„ í™œì„±í™”í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì§ˆë¬¸ì— ëŒ€í•´ ìºì‹œëœ ì‘ë‹µì„ ì¬ì‚¬ìš©í•˜ì—¬ LLM í˜¸ì¶œì„ ì¤„ì…ë‹ˆë‹¤.

    ì„¤ì •:
    - ENABLE_SEMANTIC_CACHE: ìºì‹œ í™œì„±í™” ì—¬ë¶€
    - REDIS_URL: Redis ì„œë²„ ì£¼ì†Œ
    - SEMANTIC_CACHE_DISTANCE_THRESHOLD: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.1~0.3 ê¶Œì¥)
    - SEMANTIC_CACHE_TTL: ìºì‹œ ë§Œë£Œ ì‹œê°„(ì´ˆ)
    """
    global _cache_initialized

    if _cache_initialized:
        logger.info("âœ… [Cache] RedisSemanticCacheê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    if not settings.ENABLE_SEMANTIC_CACHE:
        logger.info("â­ï¸ [Cache] SemanticCacheê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    try:
        from langchain_core.globals import set_llm_cache
        from langchain_openai import OpenAIEmbeddings
        from langchain_redis import RedisSemanticCache

        # OpenAI ì„ë² ë”© ì´ˆê¸°í™” (ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°ìš©)
        embeddings = OpenAIEmbeddings(
            api_key=settings.OPENAI_API_KEY,
            model="text-embedding-ada-002"
        )

        # RedisSemanticCache ì´ˆê¸°í™”
        semantic_cache = RedisSemanticCache(
            redis_url=settings.REDIS_URL,
            embeddings=embeddings,
            distance_threshold=settings.SEMANTIC_CACHE_DISTANCE_THRESHOLD,
            ttl=settings.SEMANTIC_CACHE_TTL,
        )

        # ì „ì—­ LLM ìºì‹œ ì„¤ì •
        set_llm_cache(semantic_cache)

        _cache_initialized = True

        logger.info(
            "âœ… [Cache] RedisSemanticCache ì´ˆê¸°í™” ì™„ë£Œ "
            "(threshold=%.2f, ttl=%ds)",
            settings.SEMANTIC_CACHE_DISTANCE_THRESHOLD,
            settings.SEMANTIC_CACHE_TTL,
        )

    except ImportError as e:
        logger.error(
            "âŒ [Cache] RedisSemanticCache ì´ˆê¸°í™” ì‹¤íŒ¨: í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. "
            "langchain-redisë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”. (%s)",
            e,
        )
    except Exception as e:
        message = str(e)
        if (
            RedisResponseError is not None
            and isinstance(e, RedisResponseError)
            and "FT._LIST" in message
        ):
            logger.info(
                "â„¹ï¸ [Cache] RediSearch ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” RedisCacheë¡œ í´ë°±í•©ë‹ˆë‹¤. (redis_url=%s)",
                settings.REDIS_URL,
            )
            try:
                import redis
                from langchain_community.cache import RedisCache

                redis_client = redis.from_url(settings.REDIS_URL)
                basic_cache = RedisCache(
                    redis_=redis_client,
                    ttl=settings.SEMANTIC_CACHE_TTL or None,
                )
                set_llm_cache(basic_cache)
                logger.info(
                    "âœ… [Cache] RedisCache ì´ˆê¸°í™” ì™„ë£Œ (key-value fallback, ttl=%s)",
                    settings.SEMANTIC_CACHE_TTL or "âˆ",
                )
            except Exception as fallback_error:
                logger.warning(
                    "âš ï¸ [Cache] RedisCache í´ë°±ì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. InMemoryCacheë¡œ ì „í™˜í•©ë‹ˆë‹¤. (%s)",
                    fallback_error,
                )
                set_llm_cache(InMemoryCache())
                logger.info("âœ… [Cache] InMemoryCache ì´ˆê¸°í™” ì™„ë£Œ (semantic cache fallback)")

            _cache_initialized = True
            return

        logger.error(
            "âŒ [Cache] RedisSemanticCache ì´ˆê¸°í™” ì‹¤íŒ¨: %s",
            message,
            exc_info=True,
        )
