"""
LLM Provider Factory

í…ŒìŠ¤íŠ¸/ê°œë°œ í™˜ê²½ì—ì„œëŠ” Geminië¥¼ ì‚¬ìš©í•˜ê³ ,
í”„ë¡œë•ì…˜/ë°ëª¨ í™˜ê²½ì—ì„œëŠ” Claudeë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìë™ ì „í™˜
"""
import asyncio
import logging
import threading
from functools import lru_cache
from typing import Optional

from langchain_anthropic import ChatAnthropic
from langchain_core.language_models import BaseChatModel
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

from src.config.settings import settings

logger = logging.getLogger(__name__)


def _loop_token() -> str:
    """
    í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„(ë˜ëŠ” ìŠ¤ë ˆë“œ)ë¥¼ ì‹ë³„í•˜ê¸° ìœ„í•œ í† í° ë°˜í™˜.

    gRPC ê¸°ë°˜ LLMë“¤ì€ ì´ë²¤íŠ¸ ë£¨í”„ì— ì¢…ì†ì ì¸ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ,
    ì„œë¡œ ë‹¤ë¥¸ ë£¨í”„ ê°„ì— ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê³µìœ í•˜ë©´ 'Event loop is closed' ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ìŠ¤ë ˆë“œ ID ê¸°ì¤€ìœ¼ë¡œ ê²©ë¦¬
        return f"sync-{threading.get_ident()}"
    return f"loop-{id(loop)}"


@lru_cache(maxsize=16)
def _build_llm(
    provider: str,
    model_name: str,
    temperature: float,
    max_tokens: int,
    loop_token: str,
) -> BaseChatModel:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ìºì‹±í•©ë‹ˆë‹¤.

    ë™ì¼í•œ ì„¤ì •(provider, model, temperature, max_tokens, loop_token)ì— ëŒ€í•´ì„œëŠ”
    ìºì‹œëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™” ë¹„ìš©ì„ ì¤„ì…ë‹ˆë‹¤.
    """
    logger.info(
        "ğŸ¤– LLM ì´ˆê¸°í™”: provider=%s, model=%s, temperature=%s, max_tokens=%s, loop=%s",
        provider,
        model_name,
        temperature,
        max_tokens,
        loop_token,
    )

    if provider == "anthropic":
        # Claude í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”
        return ChatAnthropic(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.ANTHROPIC_API_KEY,
            default_headers={"anthropic-beta": "prompt-caching-2024-07-31"},
        )

    if provider == "google":
        return ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temperature,
            max_output_tokens=max_tokens,
            google_api_key=settings.GEMINI_API_KEY,
        )

    if provider == "openai":
        return ChatOpenAI(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.OPENAI_API_KEY,
        )

    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider}")


def get_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> BaseChatModel:
    """
    LLM ëª¨ë“œì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    ê¸°ë³¸ ìš°ì„ ìˆœìœ„: OpenAI â†’ Claude (Anthropic) â†’ Gemini (Google)
    - í™˜ê²½ ë³€ìˆ˜ LLM_MODEë¡œ providerë¥¼ ëª…ì‹œí•  ìˆ˜ ìˆìŒ
    - API í‚¤ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ í´ë°±
    """
    requested_provider = settings.llm_provider
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.llm_model_name

    provider = requested_provider

    # í´ë°± ì²´ì¸: OpenAI â†’ Claude â†’ Gemini
    if provider == "openai" and not settings.OPENAI_API_KEY:
        logger.warning(
            "âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Claudeë¡œ í´ë°±í•©ë‹ˆë‹¤."
        )
        provider = "anthropic"

    if provider == "anthropic" and not settings.ANTHROPIC_API_KEY:
        logger.warning(
            "âš ï¸ ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Geminië¡œ í´ë°±í•©ë‹ˆë‹¤."
        )
        provider = "google"

    # ìµœì¢… ê²€ì¦
    if provider == "openai" and not settings.OPENAI_API_KEY:
        raise ValueError(
            "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    if provider == "anthropic" and not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    if provider == "google" and not settings.GEMINI_API_KEY:
        raise ValueError(
            "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm(provider, model_name, float(temp), int(tokens), loop_token)


def get_claude_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatAnthropic:
    """ê°•ì œë¡œ Claude ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.CLAUDE_MODEL

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_openai_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatOpenAI:
    """ê°•ì œë¡œ OpenAI ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else "gpt-5-chat-latest"

    if not settings.OPENAI_API_KEY:
        raise ValueError(
            "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("openai", model_name, float(temp), int(tokens), loop_token)


def get_gemini_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    model: Optional[str] = None,
) -> ChatGoogleGenerativeAI:
    """ê°•ì œë¡œ Gemini ì‚¬ìš© (ëª¨ë“œ ë¬´ì‹œ)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = model if model is not None else settings.GEMINI_MODEL

    if not settings.GEMINI_API_KEY:
        raise ValueError(
            "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("google", model_name, float(temp), int(tokens), loop_token)


def get_research_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Research Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_router_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Router Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_strategy_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Strategy Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_portfolio_risk_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """Portfolio/Risk Agent ì „ìš© LLM (Claude Haiku 4.5 - í”„ë¡¬í”„íŠ¸ ìºì‹± í™œì„±í™”)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)


def get_default_agent_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ChatAnthropic:
    """ê¸°ë³¸ ì—ì´ì „íŠ¸ LLM (Claude Haiku 4.5)"""
    temp = temperature if temperature is not None else settings.LLM_TEMPERATURE
    tokens = max_tokens if max_tokens is not None else settings.MAX_TOKENS
    model_name = settings.CLAUDE_MODEL

    if not settings.ANTHROPIC_API_KEY:
        raise ValueError(
            "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        )

    loop_token = _loop_token()
    return _build_llm("anthropic", model_name, float(temp), int(tokens), loop_token)
