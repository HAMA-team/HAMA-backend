from __future__ import annotations

"""
ë©€í‹° ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
Master Agent â†’ ì„œë¸Œ ì—ì´ì „íŠ¸ë“¤ì˜ í˜‘ì—… ê³¼ì •ì„ ì‹œê°í™”
"""
import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from typing import AsyncGenerator, Optional, List, Any, Dict

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig

from src.services.user_profile_service import user_profile_service
from src.services import chat_history_service
from src.services.hitl_interrupt_service import handle_hitl_interrupt
from src.models.database import get_db_context
from src.schemas.hitl_config import HITLConfig
from src.schemas.reasoning import (
    ReasoningActor,
    ReasoningEvent,
    ReasoningPhase,
    ReasoningStatus,
)
from src.config.settings import settings

logger = logging.getLogger(__name__)

router = APIRouter()

HITL_APPROVAL_TIMEOUT_SECONDS = 300
HITL_APPROVAL_POLL_INTERVAL_SECONDS = 1.0


class MultiAgentStreamRequest(BaseModel):
    """ë©€í‹° ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­"""
    message: str
    user_id: Optional[str] = None
    conversation_id: Optional[str] = None
    hitl_config: HITLConfig = Field(default_factory=HITLConfig)
    intervention_required: bool = Field(
        default=True,
        description="ë¶„ì„/ì „ëµ ë‹¨ê³„ë¶€í„° HITL í•„ìš” ì—¬ë¶€ (ê¸°ë³¸ì ìœ¼ë¡œ ë¶„ì„ë„ ìŠ¹ì¸ì„ ê¸°ë‹¤ë¦¼)",
    )
    stream_thinking: bool = Field(default=True, description="LLM ì‚¬ê³  ê³¼ì • ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™” (ChatGPTì‹)")


def _serialize_for_json(data: Any) -> Any:
    """
    LangChain ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜

    ToolMessage, AIMessage ë“± LangChain ê°ì²´ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if data is None:
        return None

    # Pydantic ëª¨ë¸ (BaseMessage ë“±)
    if hasattr(data, "model_dump"):
        try:
            return data.model_dump()
        except Exception:
            pass

    if hasattr(data, "dict"):
        try:
            return data.dict()
        except Exception:
            pass

    # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (ì¬ê·€ì ìœ¼ë¡œ)
    if isinstance(data, list):
        return [_serialize_for_json(item) for item in data]

    # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬ (ì¬ê·€ì ìœ¼ë¡œ)
    if isinstance(data, dict):
        return {key: _serialize_for_json(value) for key, value in data.items()}

    # ê¸°ë³¸ íƒ€ì… (str, int, float, bool, None)
    if isinstance(data, (str, int, float, bool, type(None))):
        return data

    # ë‚˜ë¨¸ì§€ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜ (fallback)
    try:
        return str(data)
    except Exception:
        return None


def _normalize_metadata(metadata: Any) -> Dict[str, Any]:
    if isinstance(metadata, dict):
        return metadata
    if isinstance(metadata, str):
        try:
            decoded = json.loads(metadata)
            if isinstance(decoded, dict):
                return decoded
        except json.JSONDecodeError:
            pass
    return {}


async def _await_approved_response(
    conversation_uuid: uuid.UUID,
    interrupt_time: datetime,
    timeout_seconds: float = HITL_APPROVAL_TIMEOUT_SECONDS,
    poll_interval: float = HITL_APPROVAL_POLL_INTERVAL_SECONDS,
) -> tuple[str, Dict[str, Any]] | None:
    deadline = time.monotonic() + timeout_seconds
    while time.monotonic() < deadline:
        history = await chat_history_service.get_history(
            conversation_id=conversation_uuid, limit=8
        )
        messages = history.get("messages") or []
        for message in reversed(messages):
            if getattr(message, "role", None) != "assistant":
                continue
            created_at = getattr(message, "created_at", None)
            if created_at and created_at <= interrupt_time:
                continue
            metadata = _normalize_metadata(getattr(message, "message_metadata", None))
            if metadata.get("decision"):
                text = getattr(message, "content", "") or ""
                if text.strip():
                    return text, metadata
        await asyncio.sleep(poll_interval)
    return None


def _sse(event: str, payload: dict) -> str:
    # payloadë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
    serializable_payload = _serialize_for_json(payload)
    return f"event: {event}\ndata: {json.dumps(serializable_payload, ensure_ascii=False)}\n\n"


def _event_agent_name(event: dict) -> Optional[str]:
    """
    LangGraph ì´ë²¤íŠ¸ì—ì„œ Agent ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    SubGraph ì‹¤í–‰ ì‹œ ë¶€ëª¨ Agent ì´ë¦„ì„ ì¶”ì¶œí•˜ëŠ” ë¡œì§ í¬í•¨.
    """
    metadata = event.get("metadata") or {}

    # 1. langgraph_triggersì—ì„œ ë¶€ëª¨ ë…¸ë“œ ì¶”ì  (SubGraph ì¶”ì ìš©)
    triggers = metadata.get("langgraph_triggers") or []
    if triggers:
        # triggers ì˜ˆì‹œ: ["supervisor_node", "Research_Agent"]
        # ë§ˆì§€ë§‰ triggerê°€ SubGraph ì´ë¦„ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
        for trigger in reversed(triggers):
            # "_agent" ë˜ëŠ” "_subgraph"ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ SubGraphë¡œ ê°„ì£¼
            if "_agent" in trigger.lower() or "_subgraph" in trigger.lower():
                return trigger
            # "supervisor", "master" ê°™ì€ í‚¤ì›Œë“œ ì œì™¸
            if trigger.lower() not in ["supervisor", "master", "langgraph", "supervisor_node"]:
                return trigger

    # 2. langgraph_nodeì—ì„œ SubGraph ì´ë¦„ ì¶”ì¶œ
    node = metadata.get("langgraph_node")
    if node and node != "LangGraph":
        # ë…¸ë“œ ì´ë¦„ì— "__"ê°€ ìˆìœ¼ë©´ SubGraph ì´ë¦„ ì¶”ì¶œ
        # ì˜ˆ: "Research_Agent__planner" â†’ "Research_Agent"
        if "__" in node:
            subgraph_name = node.split("__")[0]
            return subgraph_name
        return node

    # 3. event name í™•ì¸ (fallback)
    name = event.get("name")
    if name and name != "LangGraph":
        return name

    return None


def _extract_lineage(metadata: Dict[str, Any], agent: Optional[str]) -> List[str]:
    lineage: List[str] = []
    triggers = metadata.get("langgraph_triggers")
    if isinstance(triggers, list):
        lineage = [str(trigger) for trigger in triggers if trigger]
    node = metadata.get("langgraph_node")
    if node and node not in lineage:
        lineage.append(str(node))
    if agent and agent not in lineage:
        lineage.append(agent)
    return lineage


def _infer_phase(event_label: str, node: Optional[str], agent: Optional[str]) -> ReasoningPhase:
    label = (event_label or "").lower()
    node_lower = (node or "").lower()
    agent_lower = (agent or "").lower() if agent else ""

    direct_mapping = {
        "master_start": ReasoningPhase.SUPERVISION,
        "master_routing": ReasoningPhase.ROUTING,
        "master_complete": ReasoningPhase.FINALIZATION,
        "worker_start": ReasoningPhase.AGENT_EXECUTION,
        "worker_complete": ReasoningPhase.FINALIZATION,
        "hitl_interrupt": ReasoningPhase.HITL,
        "done": ReasoningPhase.FINALIZATION,
        "error": ReasoningPhase.SYSTEM,
        "user_profile": ReasoningPhase.SYSTEM,
    }
    if label in direct_mapping:
        return direct_mapping[label]
    if label.startswith("agent_llm") or label == "agent_thinking":
        return ReasoningPhase.LLM
    if label.startswith("tools_"):
        return ReasoningPhase.TOOL

    if node_lower:
        if any(keyword in node_lower for keyword in ("plan", "route", "clarify", "decision")):
            return ReasoningPhase.PLANNING
        if any(keyword in node_lower for keyword in ("data", "fetch", "collect", "search", "crawl", "ingest")):
            return ReasoningPhase.DATA_COLLECTION
    if agent_lower and "supervisor" in agent_lower:
        return ReasoningPhase.SUPERVISION
    return ReasoningPhase.AGENT_EXECUTION


def _infer_actor(event_label: str) -> ReasoningActor:
    label = (event_label or "").lower()
    if label.startswith("master") or label == "hitl_interrupt":
        return ReasoningActor.SUPERVISOR
    if label.startswith("tools_"):
        return ReasoningActor.TOOL
    if label.startswith("agent_llm") or label == "agent_thinking":
        return ReasoningActor.LLM
    if label in {"error", "done", "user_profile"}:
        return ReasoningActor.SYSTEM
    return ReasoningActor.AGENT


class ReasoningEventBuilder:
    """Utility to enrich SSE payloads with normalized reasoning metadata."""

    def __init__(self, conversation_id: str):
        self.conversation_id = conversation_id
        self._sequence = 0

    def _sequence_id(self) -> str:
        self._sequence += 1
        return f"{self.conversation_id}:{self._sequence:05d}"

    def _build_metadata(self, metadata: Optional[Dict[str, Any]], extra: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        result: Dict[str, Any] = {}
        if metadata:
            if metadata.get("langgraph_node"):
                result["langgraph_node"] = metadata["langgraph_node"]
            if metadata.get("langgraph_step") is not None:
                result["langgraph_step"] = metadata.get("langgraph_step")
            if metadata.get("langgraph_triggers"):
                result["langgraph_triggers"] = metadata.get("langgraph_triggers")
            if metadata.get("langgraph_checkpoint_id"):
                result["langgraph_checkpoint_id"] = metadata.get("langgraph_checkpoint_id")
        if extra:
            result.update(extra)
        return result

    def attach(
        self,
        event_label: str,
        payload: Dict[str, Any],
        *,
        metadata: Optional[Dict[str, Any]],
        agent: Optional[str],
        node: Optional[str],
        status: ReasoningStatus,
        message: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        metadata = metadata or {}
        lineage = _extract_lineage(metadata, agent)
        reasoning_event = ReasoningEvent(
            event_id=self._sequence_id(),
            sequence_index=self._sequence,
            conversation_id=self.conversation_id,
            event_label=event_label,
            phase=_infer_phase(event_label, node, agent),
            status=status,
            actor=_infer_actor(event_label),
            agent=agent,
            node=node,
            step=metadata.get("langgraph_step"),
            depth=max(len(lineage) - 1, 0),
            lineage=lineage,
            message=message,
            metadata=self._build_metadata(metadata, extra_metadata),
        )
        payload["reasoning_event"] = reasoning_event.model_dump()
        return payload

def _normalize_output(raw_output: Any) -> dict:
    """LangGraph ì´ë²¤íŠ¸ outputì„ dictë¡œ ì •ê·œí™”í•´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë¡œì§ì„ ë³´í˜¸."""
    if raw_output is None:
        return {}
    if isinstance(raw_output, dict):
        return raw_output
    if hasattr(raw_output, "model_dump"):
        try:
            return raw_output.model_dump()
        except Exception:
            pass
    if hasattr(raw_output, "dict"):
        try:
            return raw_output.dict()
        except Exception:
            pass
    content = getattr(raw_output, "content", None)
    if isinstance(content, dict):
        return content
    if content is not None:
        return {"message": content}
    return {}


def _event_to_sse_chunks(
    event: dict,
    stream_thinking: bool,
    reasoning_builder: ReasoningEventBuilder,
) -> List[str]:
    chunks: List[str] = []
    event_type = event.get("event")
    agent = _event_agent_name(event)

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    metadata = event.get("metadata") or {}
    node_name = metadata.get("langgraph_node")
    step = metadata.get("langgraph_step")

    # íˆ´/ì²´ì¸ ì´ë¦„
    name = event.get("name")

    # ==================== Chain ì´ë²¤íŠ¸ ====================
    if event_type == "on_chain_start" and agent:
        if agent == "routing":
            payload = reasoning_builder.attach(
                "master_routing",
                {"status": "analyzing"},
                metadata=metadata,
                agent="supervisor",
                node=node_name,
                status=ReasoningStatus.INFO,
                message="ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ë¼ìš°íŒ… ì¤‘ì…ë‹ˆë‹¤.",
            )
            chunks.append(_sse("master_routing", payload))
        elif agent == "worker_dispatch":
            payload = reasoning_builder.attach(
                "worker_start",
                {"agent": "worker"},
                metadata=metadata,
                agent="worker_dispatch",
                node=node_name,
                status=ReasoningStatus.START,
                message="ì›Œì»¤ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘",
            )
            chunks.append(_sse("worker_start", payload))
        else:
            payload = reasoning_builder.attach(
                "agent_start",
                {"agent": agent, "node": node_name, "step": step},
                metadata=metadata,
                agent=agent,
                node=node_name,
                status=ReasoningStatus.START,
                message=f"{agent} ì‹¤í–‰ ì‹œì‘",
            )
            chunks.append(_sse("agent_start", payload))

    elif event_type == "on_chain_end" and agent:
        output = _normalize_output(event.get("data", {}).get("output"))
        if agent == "routing":
            payload = reasoning_builder.attach(
                "master_routing",
                {
                    "agents": output.get("agents_to_call", []),
                    "depth_level": output.get("depth_level"),
                    "worker_action": output.get("worker_action"),
                },
                metadata=metadata,
                agent="supervisor",
                node=node_name,
                status=ReasoningStatus.COMPLETE,
                message="ì—ì´ì „íŠ¸ ë¼ìš°íŒ…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            )
            chunks.append(_sse("master_routing", payload))
        elif agent == "worker_dispatch":
            payload = reasoning_builder.attach(
                "worker_complete",
                {"result": output.get("final_response", {}), "agent": "worker"},
                metadata=metadata,
                agent="worker_dispatch",
                node=node_name,
                status=ReasoningStatus.COMPLETE,
                message="ì›Œì»¤ ì—ì´ì „íŠ¸ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            )
            chunks.append(_sse("worker_complete", payload))
        elif agent == "clarification":
            payload = reasoning_builder.attach(
                "master_complete",
                {"message": output.get("final_response", {}).get("message")},
                metadata=metadata,
                agent="supervisor",
                node=node_name,
                status=ReasoningStatus.COMPLETE,
                message="ì‚¬ìš©ì í™•ì¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ",
            )
            chunks.append(_sse("master_complete", payload))
        else:
            payload = reasoning_builder.attach(
                "agent_complete",
                {"agent": agent, "node": node_name, "step": step},
                metadata=metadata,
                agent=agent,
                node=node_name,
                status=ReasoningStatus.COMPLETE,
                message=f"{agent} ì‘ì—… ì™„ë£Œ",
            )
            chunks.append(_sse("agent_complete", payload))

    # ==================== LLM ì´ë²¤íŠ¸ ====================
    elif event_type == "on_chat_model_start" and agent:
        model = event.get("name") or event.get("data", {}).get("name")
        payload = reasoning_builder.attach(
            "agent_llm_start",
            {"agent": agent, "node": node_name, "model": model},
            metadata=metadata,
            agent=agent,
            node=node_name,
            status=ReasoningStatus.START,
            message=f"{model or 'LLM'} ë¶„ì„ ì‹œì‘",
            extra_metadata={"model": model},
        )
        chunks.append(_sse("agent_llm_start", payload))

    elif event_type == "on_chat_model_stream" and stream_thinking:
        chunk = event.get("data", {}).get("chunk")
        if chunk:
            content = chunk.get("content") if isinstance(chunk, dict) else str(chunk)
            if content:
                preview = content if len(content) <= 160 else f"{content[:160]}..."
                payload = reasoning_builder.attach(
                    "agent_thinking",
                    {"agent": agent, "node": node_name, "content": content},
                    metadata=metadata,
                    agent=agent,
                    node=node_name,
                    status=ReasoningStatus.IN_PROGRESS,
                    message=preview,
                )
                chunks.append(_sse("agent_thinking", payload))

    elif event_type == "on_chat_model_end" and agent:
        payload = reasoning_builder.attach(
            "agent_llm_end",
            {"agent": agent, "node": node_name},
            metadata=metadata,
            agent=agent,
            node=node_name,
            status=ReasoningStatus.COMPLETE,
            message="LLM ë¶„ì„ ì™„ë£Œ",
        )
        chunks.append(_sse("agent_llm_end", payload))

    # ==================== íˆ´ ì´ë²¤íŠ¸ (ìƒˆë¡œ ì¶”ê°€) ====================
    elif event_type == "on_tool_start":
        tool_name = name or "unknown_tool"
        input_data = event.get("data", {}).get("input")
        payload = reasoning_builder.attach(
            "tools_start",
            {"tool": tool_name, "agent": agent, "node": node_name, "input": input_data},
            metadata=metadata,
            agent=agent,
            node=node_name,
            status=ReasoningStatus.START,
            message=f"{tool_name} íˆ´ ì‹¤í–‰ ì‹œì‘",
            extra_metadata={"tool": tool_name},
        )
        chunks.append(_sse("tools_start", payload))

    elif event_type == "on_tool_end":
        tool_name = name or "unknown_tool"
        output_data = event.get("data", {}).get("output")

        # íˆ´ ì¶œë ¥ ë°ì´í„° ìš”ì•½ (ë„ˆë¬´ í¬ë©´ í”„ë¡ íŠ¸ì—”ë“œ ì„±ëŠ¥ ì €í•˜)
        # outputì´ ë¬¸ìì—´ì´ê³  ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì „ì†¡
        output_summary = output_data
        if isinstance(output_data, str) and len(output_data) > 500:
            output_summary = output_data[:500] + "... (truncated)"
        elif isinstance(output_data, dict):
            # ë”•ì…”ë„ˆë¦¬ëŠ” ê·¸ëŒ€ë¡œ ì „ì†¡ (ì§ë ¬í™”ëŠ” _sseì—ì„œ ì²˜ë¦¬)
            output_summary = output_data
        elif isinstance(output_data, list) and len(output_data) > 10:
            # ë¦¬ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²« 10ê°œë§Œ
            output_summary = output_data[:10]

        payload = reasoning_builder.attach(
            "tools_complete",
            {
                "tool": tool_name,
                "agent": agent,
                "node": node_name,
                "status": "complete",
                "output": output_summary,
            },
            metadata=metadata,
            agent=agent,
            node=node_name,
            status=ReasoningStatus.COMPLETE,
            message=f"{tool_name} íˆ´ ì‹¤í–‰ ì™„ë£Œ",
            extra_metadata={"tool": tool_name},
        )
        chunks.append(_sse("tools_complete", payload))

    return chunks


def _extract_final_message(state_values: dict) -> str:
    final_response = state_values.get("final_response") or {}
    if isinstance(final_response, dict):
        message = final_response.get("message")
        if isinstance(message, str) and message.strip():
            return message

    messages = state_values.get("messages") or []
    for msg in reversed(messages):
        content = getattr(msg, "content", None)
        if isinstance(content, str) and content.strip():
            return content
        if isinstance(content, list):
            text = "".join(
                part.get("text", "")
                for part in content
                if isinstance(part, dict) and part.get("type") == "text"
            )
            if text.strip():
                return text

    return "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."


async def stream_multi_agent_execution(
    message: str,
    user_id: str,
    conversation_id: str,
    hitl_config: HITLConfig,
    intervention_required: bool,
    stream_thinking: bool = True
) -> AsyncGenerator[str, None]:
    """LangGraph Supervisor ì‹¤í–‰ì„ SSEë¡œ ë˜í•‘"""
    reasoning_builder = ReasoningEventBuilder(conversation_id)
    try:
        yield _sse(
            "master_start",
            reasoning_builder.attach(
                "master_start",
                {"message": "ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤..."},
                metadata={},
                agent="supervisor",
                node=None,
                status=ReasoningStatus.START,
                message="ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”",
            ),
        )

        with get_db_context() as db:
            user_profile = user_profile_service.get_user_profile(user_id, db)

        yield _sse(
            "user_profile",
            reasoning_builder.attach(
                "user_profile",
                {"profile_loaded": True},
                metadata={},
                agent="system",
                node=None,
                status=ReasoningStatus.INFO,
                message="ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ ì™„ë£Œ",
            ),
        )

        conversation_uuid = uuid.UUID(conversation_id)
        demo_user_uuid = settings.demo_user_uuid

        await chat_history_service.upsert_session(
            conversation_id=conversation_uuid,
            user_id=demo_user_uuid,
            metadata={
                "intervention_required": intervention_required,
                "hitl_config": hitl_config.model_dump(),
            },
        )
        await chat_history_service.append_message(
            conversation_id=conversation_uuid,
            role="user",
            content=message,
        )

        conversation_history: list[dict] = []
        try:
            history_data = await chat_history_service.get_history(conversation_id=conversation_uuid, limit=10)
            if history_data and "messages" in history_data:
                messages = history_data["messages"][:-1]
                conversation_history = [
                    {"role": msg.role, "content": msg.content}
                    for msg in messages[-6:]
                ]
        except Exception as history_error:  # pragma: no cover - íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ
            logger.warning("âš ï¸ [MultiAgentStream] ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: %s", history_error)

        # ë¹„ë™ê¸° checkpointer ì‚¬ìš© (astream_eventsë¥¼ ìœ„í•´ í•„ìˆ˜)
        from src.utils.checkpointer_factory import get_checkpointer
        from src.subgraphs.graph_master import build_supervisor

        checkpointer = await get_checkpointer()
        supervisor_workflow = build_supervisor(intervention_required=intervention_required)
        app = supervisor_workflow.compile(checkpointer=checkpointer)

        config: RunnableConfig = {"configurable": {"thread_id": conversation_id}}
        configured_app = app.with_config(config)

        initial_state = {
            "messages": [HumanMessage(content=message)],
            "user_id": user_id or str(demo_user_uuid),
            "conversation_id": conversation_id,
            "hitl_config": hitl_config.model_dump(),
            "intervention_required": intervention_required,
            "user_profile": user_profile,
            "intent": None,
            "query": message,
            "agent_results": {},
            "agents_to_call": [],
            "agents_called": [],
            "risk_level": None,
            "hitl_required": False,
            "trade_prepared": False,
            "trade_approved": False,
            "trade_executed": False,
            "hitl_interrupted": False,
            "trade_order_id": None,
            "trade_result": None,
            "summary": None,
            "final_response": None,
            "conversation_history": conversation_history,
        }

        async for event in configured_app.astream_events(initial_state, version="v2"):
            for chunk in _event_to_sse_chunks(event, stream_thinking, reasoning_builder):
                yield chunk

        state = await configured_app.aget_state(config)
        pending_nodes = getattr(state, "next", None)

        if pending_nodes:
            logger.info("âš ï¸ [MultiAgentStream] Interrupt ê°ì§€: next=%s", pending_nodes)
            with get_db_context() as db:
                hitl_result = await handle_hitl_interrupt(
                    state=state,
                    conversation_uuid=conversation_uuid,
                    conversation_id=conversation_id,
                    user_id=demo_user_uuid,
                    db=db,
                    intervention_required=intervention_required,
                    hitl_config=hitl_config,
                )

            if hitl_result:
                yield _sse(
                    "hitl_interrupt",
                    reasoning_builder.attach(
                        "hitl_interrupt",
                        {
                            "pending_nodes": pending_nodes,
                            "approval_request": hitl_result["approval_request"],
                            "message": hitl_result["message"],
                        },
                        metadata={},
                        agent="supervisor",
                        node=None,
                        status=ReasoningStatus.INFO,
                        message="HITL ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    ),
                )

                interrupt_time = datetime.utcnow()
                approved_payload = await _await_approved_response(
                    conversation_uuid=conversation_uuid,
                    interrupt_time=interrupt_time,
                )

                if approved_payload:
                    final_message_text, final_metadata = approved_payload
                else:
                    logger.warning(
                        "âš ï¸ [MultiAgentStream] ìŠ¹ì¸ ê²°ê³¼ ëˆ„ë½ - íƒ€ì„ì•„ì›ƒ: %s",
                        conversation_id,
                    )
                    final_message_text = "ìŠ¹ì¸ ê²°ê³¼ê°€ ì•„ì§ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    final_metadata = {}

                yield _sse(
                    "master_complete",
                    reasoning_builder.attach(
                        "master_complete",
                        {"message": final_message_text, "conversation_id": conversation_id},
                        metadata={},
                        agent="supervisor",
                        node=None,
                        status=ReasoningStatus.COMPLETE,
                        message="ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ",
                        extra_metadata={"final_message_metadata": final_metadata},
                    ),
                )
                yield _sse(
                    "done",
                    reasoning_builder.attach(
                        "done",
                        {"conversation_id": conversation_id},
                        metadata={},
                        agent="system",
                        node=None,
                        status=ReasoningStatus.COMPLETE,
                        message="ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ",
                    ),
                )
                return
            else:  # pragma: no cover - ì˜ˆì™¸ì ì¸ ì‹¤íŒ¨
                logger.warning("âš ï¸ [MultiAgentStream] HITL í—¬í¼ ì‹¤í–‰ ì‹¤íŒ¨ - ê¸°ë³¸ ì´ë²¤íŠ¸ë§Œ ì „ì†¡")
                yield _sse(
                    "hitl_interrupt",
                    reasoning_builder.attach(
                        "hitl_interrupt",
                        {
                            "pending_nodes": pending_nodes,
                            "tasks": [getattr(task, "__dict__", str(task)) for task in getattr(state, "tasks", [])],
                        },
                        metadata={},
                        agent="supervisor",
                        node=None,
                        status=ReasoningStatus.INFO,
                        message="HITL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜",
                    ),
                )

        values = getattr(state, "values", {})
        final_message = _extract_final_message(values)

        await chat_history_service.append_message(
            conversation_id=conversation_uuid,
            role="assistant",
            content=final_message,
            metadata={"source": "graph"},
        )

        yield _sse(
            "master_complete",
            reasoning_builder.attach(
                "master_complete",
                {"message": final_message, "conversation_id": conversation_id},
                metadata={},
                agent="supervisor",
                node=None,
                status=ReasoningStatus.COMPLETE,
                message="ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ",
            ),
        )
        yield _sse(
            "done",
            reasoning_builder.attach(
                "done",
                {"conversation_id": conversation_id},
                metadata={},
                agent="system",
                node=None,
                status=ReasoningStatus.COMPLETE,
                message="ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ",
            ),
        )

    except Exception as exc:  # pragma: no cover - SSE ê²½ë¡œ ì˜¤ë¥˜ ì²˜ë¦¬
        logger.exception("âŒ [MultiAgentStream] ì‹¤í–‰ ì‹¤íŒ¨: %s", exc)
        error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}"
        yield _sse(
            "error",
            reasoning_builder.attach(
                "error",
                {"error": str(exc), "message": error_message},
                metadata={},
                agent="system",
                node=None,
                status=ReasoningStatus.ERROR,
                message="ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            ),
        )
        yield _sse(
            "done",
            reasoning_builder.attach(
                "done",
                {"conversation_id": conversation_id},
                metadata={},
                agent="system",
                node=None,
                status=ReasoningStatus.COMPLETE,
                message="ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ",
            ),
        )


@router.post("/multi-stream")
async def multi_agent_stream(request: MultiAgentStreamRequest):
    """
    ë©€í‹° ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°

    **Master Agentê°€ ì—¬ëŸ¬ ì„œë¸Œ ì—ì´ì „íŠ¸ë¥¼ ì¡°ìœ¨í•˜ëŠ” ê³¼ì •ì„ ì‹œê°í™”**

    **ì‘ë‹µ í˜•ì‹: Server-Sent Events (SSE)**

    **ì´ë²¤íŠ¸ íƒ€ì…:**
    - `master_start`: Master Agent ì‹œì‘
    - `master_routing`: ì–´ë–¤ ì—ì´ì „íŠ¸ë“¤ì„ í˜¸ì¶œí• ì§€ ê²°ì •
    - `agent_start`: ì„œë¸Œ ì—ì´ì „íŠ¸ ì‹œì‘
        - `agent`: ì—ì´ì „íŠ¸ ì´ë¦„ (ì˜ˆ: "Research_Agent", "Quantitative_Agent")
        - `node`: ë…¸ë“œ ì´ë¦„ (ì˜ˆ: "planner", "data_worker")
        - `step`: ìŠ¤í… ë²ˆí˜¸
    - `agent_llm_start`: LLM í˜¸ì¶œ ì‹œì‘
        - `agent`: ì—ì´ì „íŠ¸ ì´ë¦„
        - `node`: ë…¸ë“œ ì´ë¦„
        - `model`: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4o")
    - `agent_thinking`: LLM ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° (stream_thinking=Trueì¼ ë•Œ)
        - `agent`: ì—ì´ì „íŠ¸ ì´ë¦„
        - `node`: ë…¸ë“œ ì´ë¦„
        - `content`: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°
    - `agent_llm_end`: LLM í˜¸ì¶œ ì™„ë£Œ
        - `agent`: ì—ì´ì „íŠ¸ ì´ë¦„
        - `node`: ë…¸ë“œ ì´ë¦„
    - `tools_start`: íˆ´ ì‹¤í–‰ ì‹œì‘
        - `tool`: íˆ´ ì´ë¦„ (ì˜ˆ: "get_current_price", "get_financial_data")
        - `agent`: ì—ì´ì „íŠ¸ ì´ë¦„
        - `node`: ë…¸ë“œ ì´ë¦„
        - `input`: íˆ´ ì…ë ¥ ë°ì´í„°
    - `tools_complete`: íˆ´ ì‹¤í–‰ ì™„ë£Œ
        - `tool`: íˆ´ ì´ë¦„
        - `agent`: ì—ì´ì „íŠ¸ ì´ë¦„
        - `node`: ë…¸ë“œ ì´ë¦„
        - `status`: "complete"
        - `output`: íˆ´ ì¶œë ¥ ë°ì´í„°
    - `agent_complete`: ì„œë¸Œ ì—ì´ì „íŠ¸ ì™„ë£Œ
        - `agent`: ì—ì´ì „íŠ¸ ì´ë¦„
        - `node`: ë…¸ë“œ ì´ë¦„
        - `step`: ìŠ¤í… ë²ˆí˜¸
    - `master_complete`: ì „ì²´ ì™„ë£Œ
    - `hitl_interrupt`: HITL ìŠ¹ì¸ ìš”ì²­
    - `error`: ì—ëŸ¬ ë°œìƒ
    - `done`: ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ

    **reasoning_event ë©”íƒ€ë°ì´í„°**

    ëª¨ë“  SSE payloadì—ëŠ” `reasoning_event` ê°ì²´ê°€ í¬í•¨ë˜ë©°,
    `phase`, `status`, `depth`, `lineage`, `message` ë“±ì„ í†µí•´
    í”„ë¡ íŠ¸ì—”ë“œê°€ ì‚¬ê³  ê³¼ì •ì„ ë³´ë‹¤ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ```json
    {
      "event": "agent_start",
      "reasoning_event": {
        "event_label": "agent_start",
        "phase": "planning",
        "status": "start",
        "actor": "agent",
        "agent": "Research_Agent",
        "node": "planner",
        "depth": 1,
        "lineage": ["supervisor_node", "Research_Agent"],
        "message": "Research_Agent ì‹¤í–‰ ì‹œì‘"
      }
    }
    ```

    **Frontend ì‚¬ìš© ì˜ˆì‹œ (React):**
    ```javascript
    const [agentStatus, setAgentStatus] = useState({});
    const [currentTools, setCurrentTools] = useState([]);

    const eventSource = new EventSource('/api/v1/chat/multi-stream', {
        method: 'POST',
        body: JSON.stringify({
            message: 'ì‚¼ì„±ì „ì ë¶„ì„í•´ì¤˜',
            user_id: 'user123'
        })
    });

    eventSource.addEventListener('master_routing', (event) => {
        const data = JSON.parse(event.data);
        console.log('í˜¸ì¶œí•  ì—ì´ì „íŠ¸:', data.agents);
        // UIì— í‘œì‹œ: Research, Quantitative ì—ì´ì „íŠ¸ í™œì„±í™”
    });

    eventSource.addEventListener('agent_start', (event) => {
        const data = JSON.parse(event.data);
        console.log(`[${data.agent}] ${data.node} ë…¸ë“œ ì‹œì‘ (Step ${data.step})`);
        setAgentStatus(prev => ({
            ...prev,
            [data.agent]: 'running'
        }));
        // UI: Research Agent ì¹´ë“œì— "ì‹¤í–‰ ì¤‘ - planner" í‘œì‹œ
    });

    eventSource.addEventListener('tools_start', (event) => {
        const data = JSON.parse(event.data);
        console.log(`[${data.agent}/${data.node}] íˆ´ ì‹¤í–‰: ${data.tool}`);
        setCurrentTools(prev => [...prev, data.tool]);
        // UI: "get_financial_data ì‹¤í–‰ ì¤‘..." í‘œì‹œ
    });

    eventSource.addEventListener('tools_complete', (event) => {
        const data = JSON.parse(event.data);
        console.log(`[${data.agent}/${data.node}] íˆ´ ì™„ë£Œ: ${data.tool}`);
        setCurrentTools(prev => prev.filter(t => t !== data.tool));
        // UI: "get_financial_data ì™„ë£Œ âœ“" í‘œì‹œ
    });

    eventSource.addEventListener('agent_llm_start', (event) => {
        const data = JSON.parse(event.data);
        console.log(`[${data.agent}/${data.node}] LLM í˜¸ì¶œ: ${data.model}`);
        // UI: "AI ë¶„ì„ ì¤‘... (gpt-4o)" í‘œì‹œ
    });

    eventSource.addEventListener('agent_thinking', (event) => {
        const data = JSON.parse(event.data);
        // ì‹¤ì‹œê°„ LLM ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° (ChatGPTì²˜ëŸ¼)
        appendThinkingContent(data.agent, data.node, data.content);
    });

    eventSource.addEventListener('agent_complete', (event) => {
        const data = JSON.parse(event.data);
        console.log(`[${data.agent}] ì™„ë£Œ (Step ${data.step})`);
        setAgentStatus(prev => ({
            ...prev,
            [data.agent]: 'complete'
        }));
        // UI: Research Agent ì¹´ë“œì— "ì™„ë£Œ âœ“" í‘œì‹œ
    });

    eventSource.addEventListener('master_complete', (event) => {
        const data = JSON.parse(event.data);
        console.log('ìµœì¢… ë‹µë³€:', data.message);
        // UI: ìµœì¢… ë‹µë³€ í‘œì‹œ
    });

    eventSource.addEventListener('done', (event) => {
        eventSource.close();
    });
    ```

    **Frontend UI ì˜ˆì‹œ:**
    ```
    [Supervisor] ë¶„ì„ ì‹œì‘...
    â”œâ”€ [Routing] Research Agent ì„ íƒ âœ“
    â”‚
    â”œâ”€ ğŸ“Š [Research Agent] ì‹¤í–‰ ì¤‘...
    â”‚   â”œâ”€ [planner] ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì¤‘... âœ“
    â”‚   â”œâ”€ [data_worker] ë°ì´í„° ìˆ˜ì§‘ ì¤‘...
    â”‚   â”‚   â”œâ”€ [get_financial_data] ì¬ë¬´ ë°ì´í„° ì¡°íšŒ ì¤‘... âœ“
    â”‚   â”‚   â””â”€ [get_price_data] ê°€ê²© ë°ì´í„° ì¡°íšŒ ì¤‘... âœ“
    â”‚   â”œâ”€ [bull_worker] ê¸ì • ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ (LLM) âœ“
    â”‚   â”œâ”€ [bear_worker] ë¶€ì • ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ (LLM) âœ“
    â”‚   â””â”€ [synthesis] ì¢…í•© ë¶„ì„ (LLM) âœ“
    â”‚   â†’ ê²°ê³¼: SELL, ëª©í‘œê°€ 90,000ì›
    â”‚
    â”œâ”€ ğŸ“ˆ [Quantitative Agent] ì‹¤í–‰ ì¤‘...
    â”‚   â”œâ”€ [financial_analyzer] ì¬ë¬´ ë¶„ì„ ì¤‘...
    â”‚   â”‚   â””â”€ [calculate_ratios] PER, PBR ê³„ì‚° ì¤‘... âœ“
    â”‚   â””â”€ [valuation] ë°¸ë¥˜ì—ì´ì…˜ (LLM) âœ“
    â”‚   â†’ ê²°ê³¼: ê³ í‰ê°€ (PER 25.3)
    â”‚
    â””â”€ [Supervisor] ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘... âœ“

    âœ… ì™„ë£Œ: í˜„ì¬ ì‚¼ì„±ì „ìëŠ” ê³ í‰ê°€ êµ¬ê°„ìœ¼ë¡œ SELL ì¶”ì²œì…ë‹ˆë‹¤...
    ```
    """
    user_id = request.user_id or str(uuid.uuid4())
    conversation_id = request.conversation_id or str(uuid.uuid4())

    return StreamingResponse(
        stream_multi_agent_execution(
            message=request.message,
            user_id=user_id,
            conversation_id=conversation_id,
            hitl_config=request.hitl_config,
            intervention_required=request.intervention_required,
            stream_thinking=request.stream_thinking
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/sessions")
async def get_chat_sessions(
    limit: int = 20,
    offset: int = 0,
):
    """
    ëŒ€í™” ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ

    Args:
        limit: ì¡°íšŒí•  ì„¸ì…˜ ìˆ˜ (ê¸°ë³¸ê°’: 20)
        offset: ê±´ë„ˆë›¸ ì„¸ì…˜ ìˆ˜ (ê¸°ë³¸ê°’: 0)

    Returns:
        {
            "sessions": [
                {
                    "conversation_id": "uuid",
                    "title": "ì²« ë©”ì‹œì§€ ë‚´ìš©",
                    "last_message": "ë§ˆì§€ë§‰ ë©”ì‹œì§€",
                    "created_at": "2025-01-09T10:00:00",
                    "updated_at": "2025-01-09T10:30:00",
                    "message_count": 10
                }
            ],
            "total": 100,
            "limit": 20,
            "offset": 0
        }
    """
    try:
        # Demo ì‚¬ìš©ì UUID
        demo_user_uuid = settings.demo_user_uuid

        # ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ (ì „ì²´ ì¡°íšŒ í›„ offset ì ìš©)
        all_sessions = await chat_history_service.list_sessions(
            user_id=demo_user_uuid,
            limit=limit + offset  # offsetë§Œí¼ ë” ê°€ì ¸ì˜´
        )

        # offset ì ìš©í•˜ì—¬ ìŠ¬ë¼ì´ì‹±
        sessions_slice = all_sessions[offset:offset + limit]

        # API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_sessions = []
        for session_data in sessions_slice:
            first_msg = session_data.get("first_user_message")
            last_msg = session_data.get("last_message")
            chat_session = session_data.get("session")

            formatted_sessions.append({
                "conversation_id": str(session_data["conversation_id"]),
                "title": first_msg.content[:50] if first_msg and first_msg.content else "ìƒˆ ëŒ€í™”",
                "last_message": last_msg.content[:100] if last_msg and last_msg.content else "",
                "created_at": chat_session.created_at.isoformat() if chat_session and hasattr(chat_session, "created_at") else None,
                "updated_at": chat_session.last_message_at.isoformat() if chat_session and hasattr(chat_session, "last_message_at") and chat_session.last_message_at else None,
                "message_count": session_data.get("message_count", 0)
            })

        return {
            "sessions": formatted_sessions,
            "total": len(all_sessions),
            "limit": limit,
            "offset": offset
        }

    except Exception as e:
        logger.error(f"âŒ [ChatSessions] ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {
            "sessions": [],
            "total": 0,
            "limit": limit,
            "offset": offset
        }


@router.get("/sessions/{conversation_id}")
async def get_chat_session(conversation_id: str):
    """
    íŠ¹ì • ëŒ€í™” ì„¸ì…˜ì˜ ë©”ì‹œì§€ ì¡°íšŒ

    Args:
        conversation_id: ëŒ€í™” ID (UUID)

    Returns:
        {
            "conversation_id": "uuid",
            "messages": [
                {
                    "role": "user",
                    "content": "ì•ˆë…•í•˜ì„¸ìš”",
                    "created_at": "2025-01-09T10:00:00"
                },
                {
                    "role": "assistant",
                    "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                    "created_at": "2025-01-09T10:00:05"
                }
            ]
        }
    """
    try:
        conversation_uuid = uuid.UUID(conversation_id)
        history = await chat_history_service.get_history(
            conversation_id=conversation_uuid,
            limit=100  # ìµœê·¼ 100ê°œ ë©”ì‹œì§€
        )

        if not history:
            return {
                "conversation_id": conversation_id,
                "messages": []
            }

        # ë©”ì‹œì§€ í¬ë§·íŒ…
        messages = []
        for msg in history.get("messages", []):
            messages.append({
                "role": msg.role,
                "content": msg.content,
                "created_at": msg.created_at.isoformat() if hasattr(msg, "created_at") else None
            })

        return {
            "conversation_id": conversation_id,
            "messages": messages
        }

    except ValueError:
        logger.error(f"âŒ [ChatSession] ì˜ëª»ëœ UUID í˜•ì‹: {conversation_id}")
        return {
            "conversation_id": conversation_id,
            "messages": [],
            "error": "Invalid conversation ID format"
        }
    except Exception as e:
        logger.error(f"âŒ [ChatSession] ì„¸ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "conversation_id": conversation_id,
            "messages": [],
            "error": str(e)
        }


@router.delete("/sessions/{conversation_id}")
async def delete_chat_session(conversation_id: str):
    """
    ëŒ€í™” ì„¸ì…˜ ì‚­ì œ

    Args:
        conversation_id: ëŒ€í™” ID (UUID)

    Returns:
        {
            "success": true,
            "conversation_id": "uuid",
            "message": "ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    """
    try:
        conversation_uuid = uuid.UUID(conversation_id)

        # ì„¸ì…˜ ì‚­ì œ (delete_history ì‚¬ìš©)
        await chat_history_service.delete_history(conversation_id=conversation_uuid)

        return {
            "success": True,
            "conversation_id": conversation_id,
            "message": "ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except ValueError:
        logger.error(f"âŒ [DeleteSession] ì˜ëª»ëœ UUID í˜•ì‹: {conversation_id}")
        return {
            "success": False,
            "conversation_id": conversation_id,
            "error": "Invalid conversation ID format"
        }
    except Exception as e:
        logger.error(f"âŒ [DeleteSession] ì„¸ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "conversation_id": conversation_id,
            "error": str(e)
        }
